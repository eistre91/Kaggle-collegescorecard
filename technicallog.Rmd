---
title: "TechnicalLog"
author: "Erik Istre"
date: "December 29, 2015"
output: html_document
---

This document is primarily a reference for the developer to record the outcomes of different methods of analysis whether these analyses prove useful for the final product or not.

The dataset of interest was provided for a Kaggle competition by the US Department of Education to explore the relationship between university costs and eventual earnings. Each row (observation) in the dataset is reported information from a school for a particular year forming the columns (variables). Thus, schools will appear multiple times for each year for which data was collected from them.

```{r, warning=FALSE, message=FALSE}
setwd("C:/Kaggle-collegescorecard/")
library(ggplot2)
library(dplyr)
library(RSQLite)
library(gridExtra)

my_db <- src_sqlite("database.sqlite")
tbl <- tbl(my_db, "Scorecard")
data_dictionary <- read.csv("CollegeScorecardDataDictionary-09-12-2015.csv")
```

When first faced with the data set, it became apparent that it was too large, at 124699 observations and 1731 variables, to reliably approach with my still developing data science abilities and my limited domain knowledge. The data set at this size is also far too large to work with in local memory, and requires establishing a connection to the provided SQLite database as above.

From there I set about reading through the provided data dictionary documentation which outlined each of the variables and what they were. From there I cut out variables that, while potentially useful for an "optimal" solution, seemed like distractions for a first-pass and an alpha level product. For the initial paring down, rather than collecting the variables in one long (dplyr) select statement, I opted for more memory overhead to increase readability of the code by separating out distinct select statements and saving the resulting selections to descriptive variable names. The "long" select statement is in the next section code.

```{r}
earnings <- select(tbl, contains("_p10"), -CUML_DEBT_P10)
completion_rates <- select(tbl, C150_4_POOLED, C150_L4_POOLED, C200_4_POOLED, C200_L4_POOLED,
                                C150_4_POOLED_SUPP, C150_L4_POOLED_SUPP, C200_4_POOLED_SUPP, C200_L4_POOLED_SUPP)
retention_rates <- select(tbl, RET_FT4:RET_PTL4)
first_full_time <- select(tbl, PFTFTUG1_EF)
part_time <- select(tbl, PPTUG_EF)
enrollment <- select(tbl, UGDS)
adult_students <- select(tbl, UG25abv)
degree_breakdown <- select(tbl, PCIP01:PCIP54)
carnegie_classes <- select(tbl, CCBASIC:CCSIZSET)
cost <- select(tbl, NPT4_PUB, NPT4_PRIV, COSTT4_A:TUITIONFEE_PROG)
school_business <- select(tbl, TUITFTE:PFTFAC)
descriptive_information <- select(tbl, UNITID:LONGITUDE, Year)
admission_rate <- select(tbl, ADM_RATE)
test_scores <- select(tbl, SATVR25:SAT_AVG)
federal_aid_percent <- select(tbl, PCTFLOAN, PCTPELL)
debt_medians <- select(tbl, GRAD_DEBT_MDN, GRAD_DEBT_MDN_SUPP, WDRAW_DEBT_MDN)
operating <- select(tbl, CURROPER)
online <- select(tbl, DISTANCEONLY)
```

The future earnings in the dataset is given at a variety of time increments, e.g. 6 years from graduation and 10 years from graduation. I've opted to focus only on the 10 year data in the interest of making the analysis more manageable, and with the hope that 10 years better represents the "true" average of eventual earnings. Further, these 10 year figures were gathered in select years, and I'll be focusing on the three most recent samplings, from Year 2007, 2009, 2011.

Now that I have chosen a much smaller selection of the variables, it's possible to "collect" the data frame into local memory which makes future manipulation a bit easier to handle.

```{r}
trim <- select(tbl, contains("_p10"), -CUML_DEBT_P10, C150_4_POOLED, C150_L4_POOLED, C200_4_POOLED, C200_L4_POOLED, C150_4_POOLED_SUPP, C150_L4_POOLED_SUPP, C200_4_POOLED_SUPP, C200_L4_POOLED_SUPP, RET_FT4:RET_PTL4, PFTFTUG1_EF, PPTUG_EF, UGDS, UG25abv, PCIP01:PCIP54, CCBASIC:CCSIZSET, NPT4_PUB, NPT4_PRIV, COSTT4_A:TUITIONFEE_PROG, TUITFTE:PFTFAC, UNITID:LONGITUDE, Year, ADM_RATE, SATVR25:SAT_AVG, PCTFLOAN, PCTPELL, GRAD_DEBT_MDN, GRAD_DEBT_MDN_SUPP, WDRAW_DEBT_MDN, CURROPER, DISTANCEONLY)

trim <- collect(trim)
```

The first thing that the collection allows is an easy renaming of column names using the provided "developer friendly names" in the data dictionary that accompanies the database. The next bit of code does not work if the database was not collected, and I did attempt a more complicated method to rename the entire dataset before opting to collect and then rename.

```{r}
current_colnames <- colnames(trim)

for(i in 1:length(current_colnames)) {
  location <- which(data_dictionary$VARIABLE.NAME == current_colnames[i])
  if(length(location) == 0) {next}
  new_name <- as.character(data_dictionary[location, 4])
  if(new_name != "") {current_colnames[i] <- new_name}
}
current_colnames <- make.names(current_colnames, unique = TRUE)
colnames(trim) <- current_colnames
```

For the state column of the dataset, it contains abbreviations for U.S. territories like Puerto Rico. This is outside the main area of interest for this analysis, and so I exclude the extra state codings, including the 48 contiguous states, Alaska, Haiwaii, and DC (even though it isn't a state).

```{r}
trim <- trim %>% filter(state %in% c("AL", "AK", "AZ", "CA", "CO", "CT", "DE", "DC", "FL", "GA", "HI", "ID", "IL", "IN", "IA", "KS", "AR", "KY", "LA", "ME", "MD", "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"))
```

Some further wrangling that needs to be done is on the carnegie_basic variable which provides a classification system for the type of the schools in the dataset. This variable is only given a value on the 2013 observations. Assuming that this classification remains stable enough not to skew the analysis, I propagate the value of this variable to fill NA values where possible.

```{r}
carnegie <- trim %>% filter(Year == 2013) %>% select(name, carnegie_basic)

for(id in 1:nrow(carnegie)) {
  trim$carnegie_basic[trim$name %in% carnegie$name[id]] <- carnegie$carnegie_basic[id]
}
```

The Carnegie basic classification allows us to know a little bit more about a school in ways that are useful to our analysis. For instance, we can remove law schools and medical schools which might skew our analysis of everything else.

```{r}
normal <- filter(trim, !grepl("special", ignore.case = TRUE, carnegie_basic))
```

This completes the initial wrangling pass. More wrangling is necessary later, but this is much more context dependent, e.g. remove NA's of 10 year median earnings so that we can have a nice plot. 

The next thing I did was to explore the data set with a variety of plots. In this process I made use of two color blind friendly palettes that I found. 

```{r}
# The palette with grey:
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

# The palette with black:
cbbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

The first plot explored the relationship between the state the school was in and earnings. I'll collect up the years I'm using for the analysis first and then produce the plot.

```{r}
ten <- trim %>% filter(Year %in% c(2007, 2009, 2011))
ten_earnings <- ten %>% filter(!is.na(X10_yrs_after_entry.median), X10_yrs_after_entry.median > 0)

state_earnings <- ten_earnings %>% 
                    select(median_earnings = X10_yrs_after_entry.median, 
                           p25_earnings = pct25_earn_wne_p10,
                           p75_earnings = pct75_earn_wne_p10,
                           state)

with_median <- state_earnings %>% group_by(state) %>% summarise(median_median = median(median_earnings))
state_earnings <- mutate(state_earnings, median_median = 0)
for(i in 1:nrow(with_median)) {
  state_earnings$median_median[state_earnings$state == with_median[[i,1]]] <- with_median[[i,2]]
}
state_earnings <- state_earnings %>% arrange(median_median)

ggplot(aes(x = state), data = state_earnings) +
  geom_boxplot(aes(y = median_earnings, fill=cbPalette[2])) +
  scale_y_continuous(limits=c(0, 100000)) +
  scale_x_discrete(limits=unique(state_earnings$state)) +
  guides(fill=FALSE)
```

This plot is arranged by the median of the median 10 year earnings. DC is an obvious outlier in it's distribution of earnings.

This plot suggests discovering whether the difference in 10 year earnings is simply due to differences in cost of living (e.g. DC is expensive to live and drives incomes higher) or whether there may be some more fundamental difference going on between states. One way to determine this might be normalizing by cost of living. This analysis isn't perfect though. There is a large degree of mobility (mobility.R) among college graduates, and I couldn't find a dataset to nail down the proportions of people moving for school and then staying the state they go to school for, or people moving post graduation. 

However, if I rephrase the question to be "how much of the variance in median earnings is explained by differences in cost of living", I might still gather some insight. Thus we need a dataset which gives cost of living indexes for the different states (and DC).

```{r}
relative <- read.csv("relativevalue.csv", skipNul = TRUE)
colnames(relative) <- c("state", "relative")

state_earnings <- state_earnings %>% mutate(relative_value = 0)
for(i in 1:nrow(with_median)) {
  state_earnings$relative_value[state_earnings$state == relative[[i,1]]] <- relative[[i,2]]
}

state_earnings <- state_earnings %>% mutate(adjusted_earnings = (median_earnings/1000)*(relative_value/100))

with_median <- state_earnings %>% group_by(state) %>% summarise(median_adjusted = median(adjusted_earnings))
state_earnings <- mutate(state_earnings, median_adjusted = 0)
for(i in 1:nrow(with_median)) {
  state_earnings$median_adjusted[state_earnings$state == with_median[[i,1]]] <- with_median[[i,2]]
}
state_earnings <- state_earnings %>% arrange(median_adjusted)

ggplot(aes(x = state), data = state_earnings) +
  geom_boxplot(aes(y = adjusted_earnings, fill=cbPalette[2])) +
  scale_y_continuous(limits=c(20,50)) +
  scale_x_discrete(limits=unique(state_earnings$state)) +
  guides(fill=FALSE)

#the scale is skewing the medians for some states
#should shift to the coord_trans zoom
```

Another bit of code that I tried to get working, but ultimately found to be a lot more difficult, was building a "simple" linear regression model. I was hoping to something rudimentary up and running with a few variables to get an idea of what might be strongly predictive of higher later earnings. However, I quickly ran into issues with the amounts of NAs present in this dataset. 

I was particularly curious about the relationship between SAT/ACT scores and eventual earnings, but there are quite a few schools in the dataset that haven't reported this information, and this made the analysis with these difficult and unreliable. 

I had also planned to train on years 2007, 2009 and then test on 2011, which only further limited the available data once NAs were accounted for. At this time, the attempt to build a linear regression model has been put on hold.

Attempting K-means clustering on percentages of degrees.

```{r}
degree_breakdown <- collect(degree_breakdown)
degree_breakdown_filter <- trim %>% select(contains("program_percentage")) %>% filter(!is.na(program_percentage.agriculture))

KMC <- kmeans(degree_breakdown_filter, centers=9, iter.max=1000)

SumWithinss = sapply(2:10, 
                     function(x) 
                       sum(kmeans(degree_breakdown_filter, centers=x, iter.max=1000)$withinss))

plot(2:10, SumWithinss, type="b")

#need at least 4 clusters.
#another relatively sharp decrease from 8 to 9 clusters, and then
#an increase to 10 
```